{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Workplace Feedback Data with Python\n",
    "\n",
    "#### Code Highlights\n",
    "- We use the `pandas` library to read and manipulate the Excel data.\n",
    "- The `openpyxl` library is installed to ensure compatibility with Excel files.\n",
    "- The data is loaded into a DataFrame for easy analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/{directory}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Highlights\n",
    "- We will extract a subset of columns that are relevant to our analysis.\n",
    "- The `re` library, which allows for text processing or data cleaning steps in the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[[\"Company_Name\",\"Review\", \"Overall_Job_Satisfaction\", \"Imporvement_Needed\", \"Flexibility_Rating\", \"Manager_Support\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation from Review column\n",
    "for col in [\"Review\"]:\n",
    "    data[col] = data[col].map(lambda x: re.sub('[,\\.!?]', '', str(x)))\n",
    "\n",
    "# Convert the text in Review column to lowercase\n",
    "for col in [\"Review\"]:\n",
    "    data[col] = data[col].map(lambda x: str(x).lower())\n",
    "\n",
    "# Print out the first few rows of the processed column\n",
    "print(data[[\"Review\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with empty strings from your DataFrame\n",
    "data_cleaned = data[(data != '').all(axis=1)]\n",
    "\n",
    "# Now, data_cleaned should not contain rows with empty strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenization and Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Modeling**: Gensim provides powerful algorithms like Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA) for extracting meaningful topics from collections of documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to tokenize words and remove punctuation\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield simple_preprocess(str(sentence), deacc=True)  # deacc=True removes punctuations\n",
    "\n",
    "\n",
    "#DataExtraction\n",
    "data = []\n",
    "for col in [\"Review\"]:\n",
    "    data.extend(data_cleaned[col].values.tolist())\n",
    "\n",
    "# Tokenize and clean up the text\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Print the first 30 words from the processed text\n",
    "print(data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Modeling: Bigram and Trigram Models\n",
    "\n",
    "Implementing bigram and trigram models involves tokenizing text into words and counting word pairs (bigrams) or triplets (trigrams). Probability calculations estimate word sequence likelihoods. Libraries like NLTK (Natural Language Toolkit) and spaCy in Python provide tools for working with bigrams and trigrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pydantic typing-extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list | grep typing-extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from gensim.utils import simple_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NLTK stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['from', 'subject', 're', 'edu', 'use', 'food', 'foods', 'foodservice', 'lot', 'sage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams, and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    bigram_phrases = []\n",
    "    for doc in texts:\n",
    "        bigrams = list(ngrams(doc, 2))\n",
    "        bigram_phrases.append([f\"{word1}_{word2}\" for word1, word2 in bigrams])\n",
    "    return bigram_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams(texts):\n",
    "    trigram_phrases = []\n",
    "    for doc in texts:\n",
    "        trigrams = list(ngrams(doc, 3))\n",
    "        trigram_phrases.append([f\"{word1}_{word2}_{word3}\" for word1, word2, word3 in trigrams])\n",
    "    return trigram_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is a natural language processing (NLP) technique that reduces words to their base or root form, often used to normalize text for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    lemmatized_texts = []\n",
    "    for sent in texts:\n",
    "        lemmatized_sent = []\n",
    "        for word in sent:\n",
    "            # Replace this with your own lemmatization logic if needed\n",
    "            lemmatized_sent.append(word)  # No lemmatization here, just keeping the words\n",
    "        lemmatized_texts.append(lemmatized_sent)\n",
    "    return lemmatized_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Lemmatization (No spaCy)\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Gensim Corpus\n",
    "\n",
    "In natural language processing (NLP), a **corpus** is a collection of documents represented in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus (List of Lists)\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence Score Calculation\n",
    "\n",
    "In topic modeling, the **coherence score** is a metric used to evaluate the quality and interpretability of the topics generated by a topic modeling algorithm, such as Latent Dirichlet Allocation (LDA). It provides a quantitative measure of how coherent and meaningful the topics are within a given corpus of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(corpus, dictionary, k, alpha, beta, chunksize=100, passes=10):\n",
    "    \"\"\"\n",
    "    Compute the coherence value for a given LDA model.\n",
    "\n",
    "    Parameters:\n",
    "    - corpus: The corpus in bag-of-words format.\n",
    "    - dictionary: The dictionary mapping words to IDs.\n",
    "    - k: Number of topics.\n",
    "    - alpha: Alpha hyperparameter.\n",
    "    - beta: Beta hyperparameter.\n",
    "    - chunksize: Chunk size for training (optional, default=100).\n",
    "    - passes: Number of passes for training (optional, default=10).\n",
    "\n",
    "    Returns:\n",
    "    - Coherence value for the LDA model.\n",
    "    \"\"\"\n",
    "    lda_model = gensim.models.LdaMulticore(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=k,\n",
    "        random_state=100,\n",
    "        chunksize=chunksize,\n",
    "        passes=passes,\n",
    "        alpha=alpha,\n",
    "        eta=beta\n",
    "    )\n",
    "\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "\n",
    "    return coherence_model.get_coherence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store coherence scores\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 6\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, .5, 0.1))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs * 0.75)), corpus]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the grid of hyperparameters\n",
    "for k in topics_range:\n",
    "    for a in alpha:\n",
    "        for b in beta:\n",
    "            # Check if 'a' is a string, and if so, use it as is, otherwise, convert it to a float\n",
    "            alpha_value = a if isinstance(a, str) else float(a)\n",
    "            \n",
    "            # Check if 'b' is a string, and if so, use it as is, otherwise, convert it to a float\n",
    "            beta_value = b if isinstance(b, str) else float(b)\n",
    "            \n",
    "            # Compute coherence score for the current combination\n",
    "            cv = compute_coherence_values(corpus, id2word, k, alpha_value, beta_value)\n",
    "            \n",
    "            # Store the coherence score in the grid\n",
    "            if k not in grid:\n",
    "                grid[k] = {}\n",
    "            grid[k][f'alpha={a}, beta={b}'] = cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory where you want to save the CSV file\n",
    "output_directory = 'Users/{directory}/results'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Save the results to a CSV file in the specified directory\n",
    "csv_file_path = os.path.join(output_directory, 'lda_tuning_results_fairy.csv')\n",
    "pd.DataFrame(model_results).to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus, id2word, k, alpha_value, beta_value)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    ##pd.DataFrame(model_results).to_csv('./{directory}.csv', index=False)\n",
    "    pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired parameters\n",
    "num_topics = 4  # Number of topics\n",
    "alpha = 0.11    # Alpha hyperparameter\n",
    "eta = 0.31      # Eta hyperparameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=num_topics,\n",
    "    random_state=100,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha=alpha,\n",
    "    eta=eta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = './{directory}/results/fairy_ldavis_tuned_' + str(num_topics) + '.html'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a bit time-consuming - make the if statement True\n",
    "# if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# Load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "# Save the HTML file for visualization (optional)\n",
    "pyLDAvis.save_html(LDAvis_prepared, LDAvis_data_filepath)\n",
    "\n",
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
